FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=10 10 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (10, 0, 5.00000000000000000000e-01) (10, 0, 5.00000000000000000000e-01) (10, 0, 5.00000000000000000000e-01) (10, 0, 5.00000000000000000000e-01) (10, 0, 5.00000000000000000000e-01) (10, 0, 5.00000000000000000000e-01) (10, 0, 5.00000000000000000000e-01) (10, 0, 5.00000000000000000000e-01) (10, 0, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (10, 4, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 1.54171139001846313477e-02) (1, -8.32841694355010986328e-02) (2, 5.63094094395637512207e-02) (3, -4.06322367489337921143e-02) (4, 9.63660106062889099121e-02) (5, 2.05599814653396606445e-02) (6, 8.07036533951759338379e-02) (7, 6.24455511569976806641e-04) (8, -2.73187458515167236328e-02) (9, 6.13892897963523864746e-02) (0, 8.42580273747444152832e-02) (1, -5.52918016910552978516e-02) (2, 6.08281567692756652832e-02) (3, 4.27664294838905334473e-02) (4, 7.99698755145072937012e-02) (5, -3.02032977342605590820e-02) (6, -4.51425202190876007080e-02) (7, 1.52226537466049194336e-03) (8, 2.92394310235977172852e-03) (9, 3.19340080022811889648e-03) (0, 2.98529341816902160645e-02) (1, -7.73261487483978271484e-02) (2, -3.78587655723094940186e-02) (3, 3.59742864966392517090e-02) (4, -6.49022012948989868164e-02) (5, -7.83745944499969482422e-03) (6, 6.37185052037239074707e-02) (7, 8.25124457478523254395e-02) (8, 3.77626940608024597168e-02) (9, -2.79565826058387756348e-02) (0, 6.23146221041679382324e-02) (1, -4.68202009797096252441e-02) (2, -1.12407505512237548828e-02) (3, 1.86240226030349731445e-02) (4, 1.25475674867630004883e-02) (5, -1.48747414350509643555e-02) (6, -6.08160085976123809814e-02) (7, -6.74878060817718505859e-03) (8, 8.57497230172157287598e-02) (9, 1.18652433156967163086e-02) (0, -4.53594885766506195068e-02) (1, 7.00077489018440246582e-02) (2, 5.65734580159187316895e-02) (3, -8.45313370227813720703e-02) (4, 1.27741694450378417969e-02) (5, 3.65433171391487121582e-02) (6, -1.47346332669258117676e-02) (7, 6.76316544413566589355e-02) (8, -6.19344115257263183594e-02) (9, 8.81893113255500793457e-02) (0, -2.91749387979507446289e-02) (1, 6.79185166954994201660e-02) (2, -8.91368463635444641113e-02) (3, 3.29662933945655822754e-02) (4, 3.89281660318374633789e-03) (5, -5.40390424430370330811e-02) (6, -7.48711600899696350098e-02) (7, -3.23886871337890625000e-02) (8, -7.15265870094299316406e-02) (9, 6.28915354609489440918e-02) (0, 3.96547392010688781738e-02) (1, 9.07880142331123352051e-02) (2, -8.39286744594573974609e-02) (3, -7.15860202908515930176e-02) (4, 9.41203534603118896484e-03) (5, 2.86188796162605285645e-02) (6, 1.35392397642135620117e-02) (7, 4.85960319638252258301e-02) (8, -7.81298950314521789551e-02) (9, -7.11038708686828613281e-04) (0, -3.95387187600135803223e-02) (1, -2.34893783926963806152e-02) (2, -3.07032912969589233398e-02) (3, -8.29652696847915649414e-02) (4, -8.02071392536163330078e-03) (5, 8.20708796381950378418e-02) (6, 5.35780563950538635254e-02) (7, 7.72446468472480773926e-02) (8, 4.97025325894355773926e-02) (9, 9.16436389088630676270e-02) (0, 6.54339715838432312012e-02) (1, -7.94724151492118835449e-02) (2, 5.95621541142463684082e-02) (3, 7.62971267104148864746e-02) (4, 5.34938797354698181152e-02) (5, -3.65450307726860046387e-02) (6, -7.77419209480285644531e-02) (7, 7.86227062344551086426e-02) (8, 3.10662910342216491699e-02) (9, -4.92685101926326751709e-02) (10, 4.15142402052879333496e-02) (11, -2.92789787054061889648e-02) (12, -5.84804825484752655029e-02) (13, 5.75855746865272521973e-02) (14, -8.64997506141662597656e-04) (15, 5.09315654635429382324e-02) (16, -1.37955397367477416992e-02) (17, -8.73257592320442199707e-02) (18, -4.72404062747955322266e-04) (19, 8.07455927133560180664e-03) 
